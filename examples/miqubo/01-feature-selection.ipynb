{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection with the D-Wave System\n",
    "Placeholder sentence: In the [feature selection demo](<<< leap_url >>>/demos/socialnetwork/) you saw ...   \n",
    "\n",
    "This notebook examines how you can solve optimization problems on a  D-Wave quantum processing unit (QPU) with the example of a feature-selection problem.\n",
    "    \n",
    "1. [What is Feature Selection?](#What-is-Feature-Selection?) defines and explains the feature-selection problem.\n",
    "2. [Feature Selection by Mutual Information](#Feature-Selection-by-Mutual-Information) describes a particular method of feature selection that is used in this notebook.\n",
    "3. [Solving Feature Selection on a Quantum Computer](#Solving-Feature-Selection-on-a-Quantum-Computer) shows how such optimization problems can be formulated for solution on a quantum computer. \n",
    "4. [Example Application: Predicting Survival of Titanic Passangers](#Example-Application:-Predicting-Survival-of-Titanic-Passangers) uses feature selection on a known problem. \n",
    "\n",
    "This notebook should help you understand both the techniques and [Ocean software](https://github.com/dwavesystems) tools used for solving optimization problems on D-Wave quantum computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**New to Jupyter Notebooks?** JNs are divided into text or code cells. Pressing the **Run** button in the menu bar moves to the next cell. Code cells are marked by an \"In: \\[\\]\" to the left; when run, an asterisk displays until code completion: \"In: \\[\\*\\]\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Feature Selection?\n",
    "Statistical and machine-learning models use a set of input variables (features)\n",
    "to predict output variables of interest. Feature selection, which can be\n",
    "part of the model design process, simplifies the model and reduces dimensionality by selecting,\n",
    "from a given set of potential features, a subset of highly informative ones. \n",
    "\n",
    "For example, if Farmer Jones were creating a model for predicting the ripening of her hothouse tomatoes, she might start recording daily the following list of potential features: date, air temperature, degree of cloudiness, hours of daylight, daily water, fertilizer, air humidity, hours of electric light, ambient music style. After a growth season or two, she analyzes correlations between these features and her tomato crops. Her analysis reveals:\n",
    "\n",
    "* date, cloudiness, daylight have little predictive power\n",
    "* water and humidity are highly predictive of crop rot; they are also highly correlated (the hothouse has a roof sprinkler) \n",
    "* fertilizer is highly predictive of fruit size\n",
    "\n",
    "Farmer Jones understands that her hothouse's electric light makes her crop less dependant on seasons (date) and sunshine (cloudiness). She can simplify her model by disregarding those features. She can also reduce the number of inputs by recording either her water or humidity measurement but not both.\n",
    "\n",
    "For systems with large amounts of potential input information, such as weather forecasting or facial recognition, the model complexity and required compute resources can be daunting. Feature selection can help make such models tractable. \n",
    "\n",
    "However, optimal feature selection itself can be a hard problem. This example introduces a powerful method of optimizing feature selection based on a hard probability calculation. To overcome the difficulties of this calculation, it formulates a solution by quantum computer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustrative Toy Problem\n",
    "This section illustrates the use of feature selection with a very simple example.\n",
    "\n",
    "An example system has a single output that is generated by three inputs. The model then used to predict the output is even simpler than the system, with just two features. If all three inputs contribut somewhat commensurately* to the output, the model should perform better if the selected features are more independant. If this was a model of Farmer Jones's tomatoes, selecting water and fertilizer should be better than water and humidity.\n",
    "\n",
    "*: If an independant input contributes less than the difference between two correlated ones, this might not be true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first code cell creates three inputs, two of which are very similar---a sine and a noisy sine---and one that is linear with added random noise.  For simplicity, the output is a linear combination of the three inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sig_len = 100\n",
    "# Three features: in1 & in2 are similar; in3 is different \n",
    "in1 = np.sin(np.linspace(-np.pi, np.pi, sig_len)).reshape(sig_len, 1)\n",
    "in2 = np.sin(np.linspace(-np.pi+0.1, np.pi+0.2, sig_len)).reshape(sig_len, 1) + 0.3*np.random.rand(sig_len, 1)\n",
    "in3 = np.linspace(-1, 1, sig_len).reshape(sig_len,1) + 2*np.random.rand(sig_len, 1)\n",
    "\n",
    "# Variable of interest\n",
    "out = 2*in1 + 3*in2 + 6*in3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the features (three inputs) and the variable of interest (the output). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from helpers.plots import plot_toy_signals # To see helper functions, select Jupyter File Explorer View from the Online Learning page\n",
    "\n",
    "# Store problem in a pandas DataFrame for later use\n",
    "toy = pd.DataFrame(np.hstack((in1, in2, in3, out)), columns=[\"in1\", \"in2\", \"in3\", \"out\"])\n",
    "\n",
    "plot_toy_signals(toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that inputs 1 and 2 are much more correlated to each other than either of those two are correlated to input 3.\n",
    "\n",
    "The `two_var_model` function in the cell below defines a simple linear model of only two variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple two-variable model\n",
    "def two_var_model(IN, a, b):\n",
    "    ina, inb = IN\n",
    "    return a*ina + b*inb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use SciPy's `curve_fit` function to try to reproduce the variable of interest from just two of three features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "import itertools\n",
    "from helpers.plots import plot_two_var_model # To see helper functions, select Jupyter File Explorer View from the Online Learning page\n",
    "\n",
    "model = []\n",
    "two_vars = []\n",
    "for f0, f1 in itertools.combinations(['in1', 'in2', 'in3'], 2):  \n",
    "    popt, pcov = curve_fit(two_var_model, (toy[f0].values, toy[f1].values), toy['out'].values)\n",
    "    model.append(two_var_model((toy[f0].values, toy[f1].values), popt[0], popt[1]).reshape(len(toy), 1))\n",
    "    two_vars.append((f0, f1))\n",
    "    print(\"Standard deviation for selection of features {} and {} is {:.4f}.\".format(f0, f1, max(np.sqrt(np.diag(pcov)))))\n",
    "model_df = pd.DataFrame(np.hstack(model), columns=two_vars)\n",
    "\n",
    "plot_two_var_model(model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model based on less-correlated features perfoms better.\n",
    "\n",
    "The following sections can help you benefit from the advantages of optimal feature selection in modeling more complex systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection by Mutual Information\n",
    "There are many methods to do feature selection. For example, if you are building a deep learning network and have six potential features, you might try training first on each of the features by itself, then on all 15 combinations of subsets of two features, then 20 combinations of subsets of three features, and so on. This naive method rarely works for real-world cases. For those, statistical methods are much more efficient.  \n",
    "\n",
    "One statistical criterion that can guide this selection is mutual information (MI). The following subsections explain information and MI.\n",
    "\n",
    "If you already understand mutual information and Shannon entropy, please skip to section [Solving Feature Selection on a Quantum Computer](#Solving-Feature-Selection-on-a-Quantum-Computer). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying Information: Shannon Entropy\n",
    "How does Shannon entropy (SE) quantify information in a signal? $H(X)$, a measure of information in a signal\n",
    "\n",
    "$H(X) = - \\sum_{x \\in X} p(x) \\log p(x)$\n",
    "\n",
    "The formula for SE can be viewed as weighing the possible values $\\log \\frac{1}{p(x)}$ by the probability of occurring, $p(x)$. **Some more explanation here...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate SE, this section defines a probablity function that takes the data of some variables (this would be training data in a machine learning context) and divides it into bins to map the probability of any data point (as a statistical event)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob(dataset, max_bins=10):\n",
    "    \"\"\"Joint probability distribution P(X) for the given data.\"\"\"\n",
    "\n",
    "    # bin by the number of different values per feature\n",
    "    num_rows, num_columns = dataset.shape\n",
    "    bins = [min(len(np.unique(dataset[:, ci])), max_bins) for ci in range(num_columns)]\n",
    "\n",
    "    prob, _ = np.histogramdd(dataset, bins)\n",
    "    return prob / np.sum(prob)\n",
    "\n",
    "def shannon_entropy(p):\n",
    "    \"\"\"Shannon entropy H(X) is the sum of P(X)log(P(X)) for probabilty distribution P(X).\"\"\"\n",
    "    p = p.flatten()\n",
    "    return -sum(pi*np.log2(pi) for pi in p if pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration of Shannon Entropy\n",
    "To see an intuitive example of measuring SE, this subsection applies it to three signals with defined distributions:\n",
    "\n",
    "* Uniform: all outcomes are equally likely, or unpredictable, so maximal values of SE occur for uniform distribution: $H(X) = log(N)$ where $N$ is the number of possible outcomes, ${x_1, x_2, ...x_N}$. \n",
    "* Exponential: for increasing on curve steepness, an outcome further along the tail has increasing probability, so lower information for more of the outcomes. \n",
    "* Unequal probabilities: outcomes with higher probabilities have lower information&mdash;on average, we're learning less from each event. Biasing this signal strongly to one outcome means most the time the signal is predictable, or low in information. $H(X) = -p \\log(p) - (1-p) \\log(1-p)$ so for $p = 0.1$, for example, $H(X) = 0.468$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plots import plot_se # To see helper functions, select Jupyter File Explorer View from the Online Learning page\n",
    "\n",
    "max_bins = 10\n",
    "\n",
    "# Signals with well-defined distributions\n",
    "x_uniform = np.random.uniform(0, 1, (1000, 1))\n",
    "x_exp = np.exp(-np.linspace(0, 10, 1000)/2).reshape(1000, 1)\n",
    "x_vals = np.random.choice([0, 1],(1000, 1), p=[0.1, 0.9])\n",
    "\n",
    "data = list()\n",
    "for bins in range(1, max_bins):\n",
    "    uniform_se = shannon_entropy(prob(x_uniform, bins))\n",
    "    exp_se = shannon_entropy(prob(x_exp, bins))\n",
    "    vals_se = shannon_entropy(prob(x_vals, bins))                               \n",
    "    data.append({'Bins': bins, 'Uniform': uniform_se, 'Maximum': np.log2(bins), 'Exp': exp_se, 'Vals': vals_se})\n",
    "\n",
    "plot_se(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Shannon Entropy\n",
    "\n",
    "Conditional SE (CSE) measures the information in a signal, $X$, if the value of signal $Y$ is known: \n",
    "\n",
    "$\\begin{aligned} H(X|Y) \n",
    "& = H(X,Y)-H(Y) \\\\\n",
    "& = - \\sum_{x \\in X} p(x, y) \\log p(x, y) - H(Y) \\end{aligned}$\n",
    "\n",
    "where $H(X,Y)$, the joint SE, measures the information in both signals $X$ and $Y$ together, and $p(x,y)$ is the joint probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_shannon_entropy(p, *conditional_indices):\n",
    "    \"\"\"entropy of P conditional on variable j\"\"\"\n",
    "\n",
    "    axis = tuple(i for i in np.arange(len(p.shape)) if i not in conditional_indices)\n",
    "\n",
    "    return shannon_entropy(p) - shannon_entropy(np.sum(p, axis=axis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustration of CSE \n",
    "Apply CSE to the toy problem: Signals `in1` and `in2` being similar, knowing the value of one provides a good estimate of the value of the other. In contrast, the value of signal `in3` provides a less accurate estimate of the first two.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"H(in1) = {:.2f}\".format(shannon_entropy(prob(toy[[\"in1\"]].values))))\n",
    "print(\"H(in1|in3) = {:.2f}\".format(conditional_shannon_entropy(prob(toy[[\"in1\", \"in3\"]].values), 1)))\n",
    "print(\"H(in1|in2) = {:.2f}\".format(conditional_shannon_entropy(prob(toy[[\"in1\", \"in2\"]].values), 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information\n",
    "At a high level, MI quantifies how much one knows about one random variable from observations of another variable. Intuitively, if two features have high MI, a model based on just one of the features should be closer to a model based on both compared to features with low MI. \n",
    "\n",
    "MI between variables $X$ and $Y$, is defined as \n",
    "\n",
    "$I(X;Y)  = \\sum_{y \\in Y} \\sum_{x \\in X} p(x, y) \\log \\frac{p(x,y)}{p(x)p(y)}$\n",
    "\n",
    "where $p(x)$ and $p(y)$ are marginal probabilities of $X$ and $Y$, and $p(x,y)$ the joint probability. Equivalently, \n",
    "\n",
    "$I(X;Y)  = H(Y) - H(Y|X)$\n",
    "\n",
    "where $H(Y)$ is the SE of $Y$ and $H(Y|X)$ is the CSE of $Y$ conditional on $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(prob, j):\n",
    "    \"\"\"mutual information between all variables and variable j\"\"\"\n",
    "    return shannon_entropy(np.sum(prob, axis=j)) - conditional_shannon_entropy(prob, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MI on the Toy Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helpers.plots import plot_mi # To see helper functions, select Jupyter File Explorer View from the Online Learning page\n",
    "\n",
    "scores = {}\n",
    "for column in toy.columns:\n",
    "    if column == 'out':\n",
    "        continue\n",
    "    scores[column] = mutual_information(prob(toy[['out', column]].values), 0)\n",
    "\n",
    "plot_mi(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your impression from from looking at the input and output signals of the toy problem in the first section was that the two sine signals seem closer to the output than `in3`, run the linear regression below to verify the MI result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "from helpers.plots import plot_lingress # To see helper functions, select Jupyter File Explorer View from the Online Learning page\n",
    "\n",
    "model = []\n",
    "var_rval = []\n",
    "for column in toy.columns:\n",
    "    if column == 'out':\n",
    "        continue\n",
    "    slope, intercept, rvalue, *_ = linregress(toy[column].values, toy['out'].values)  \n",
    "    model.append((slope*toy[column].values + intercept).reshape(len(toy), 1))\n",
    "    var_rval.append((column, rvalue))\n",
    "\n",
    "plot_lingress(pd.DataFrame(np.hstack(model), columns=var_rval), toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Mutual Information\n",
    "\n",
    "$I(X_j;Y |X_i)$ represents conditional MI (CMI) between the output variable and a feature given the selection of another feature\n",
    "\n",
    "Last, the next cell defines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_mutual_information(p, j, *conditional_indices):\n",
    "    \"\"\"Mutual information between variables X and variable Y conditional on variable Z.\n",
    "    Calculated as I(X;Y|Z) = H(X|Z) - H(X|Y,Z)\"\"\"\n",
    "\n",
    "    return conditional_shannon_entropy(np.sum(p, axis=j), *conditional_indices) - conditional_shannon_entropy(p, j, *conditional_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply CMI to the toy problem: Signals `in1` and `in2` being similar, knowing the value of one provides a good estimate of the value of the other. In contrast, the value of signal `in3` provides a less accurate estimate of the first two.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"I(out;in1|in2) = {:.2f}\".format(conditional_mutual_information(prob(toy[['out', 'in1', 'in2']].values), 1, 2)))\n",
    "print(\"I(out;in1|in3) = {:.2f}\".format(conditional_mutual_information(prob(toy[['out', 'in1', 'in3']].values), 1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, to select the $k$ most relevant features of $n$ for a model, you could maximize $I({X_s}; Y)$, the MI between a set of $k$ features, $X_s$, and the output variable of interest, $Y$. This is a hard calculation because $n \\choose k$ grows rapidly in real-world problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Feature Selection on a Quantum Computer\n",
    "There are different methods of approximating the hard calculation of $n \\choose k$ features based on maximizing MI. One approach assumes conditional independence of the features and limits MI calculations to sets of three features. The optimal set of features is then given by:\n",
    "\n",
    "$\\frac{1}{k} \\arg \\max_s \\sum_{i=1}^n \\left \\{ I(X_i;Y) + \\sum_{j \\in s|_i} I(X_j;Y |X_i) \\right \\}$\n",
    "\n",
    "\n",
    "The lefthand component, $I(X_i;Y)$, represents MI between the output variable and a particular feature; maximizing means selecting features that are highly predictive of the output. The righthand component, $I(X_j;Y |X_i)$, represents conditional MI between the output variable and a feature given the selection of another feature; maximizing means selecting features that complement information about the output variable rather than provide redundant information.\n",
    "\n",
    "This approximation is still a hard calculation; this section demonstrates a method for formulating it for solution on the D-Wave quantum computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIQUBO\n",
    "D-Wave systems solve binary quadratic models (BQM), the Ising model traditionally used in statistical mechanics and its computer-science equivalent, the quadratic unconstrained binary optimization (QUBO) problem. Given $N$ variables $x_1,...,x_N$, where each variable $x_i$ can have binary values $0$ or $1$, the system finds assignments of values that minimize,\n",
    "    \n",
    "$\\sum_i^N q_ix_i + \\sum_{i<j}^N q_{i,j}x_i  x_j$,\n",
    "    \n",
    "where $q_i$ and $q_{i,j}$ are configurable (linear and quadratic) coefficients. To formulate a problem for the D-Wave system is to program $q_i$ and $q_{i,j}$ so that assignments of $x_1,...,x_N$ also represent solutions to the problem.\n",
    "\n",
    "For feature selection, the Mutual Information QUBO (MIQUBO) method formulates a QUBO based on the approximation above for $I({X_s}; Y)$, which is submitted to the D-Wave quantum computer for solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulating a QUBO\n",
    "The approximation above for optimal feature selection, with its limitation on calculating MI to sets of three variables, is a natural fit for QUBO formulation. \n",
    "\n",
    "<table style=\"width:70%\">\n",
    "  <tr>\n",
    "    <th width=\"50%\">Formula</th>\n",
    "    <th width=\"10%\">Optimization</th> \n",
    "    <th width=\"15%\">Linear terms</th>\n",
    "    <th width=\"15%\">Quadratic terms</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td><b>Feature Selection</b>: $\\sum_{i=1}^n \\left \\{ I(X_i;Y) + \\sum_{j \\in s|_i} I(X_j;Y |X_i) \\right \\}$</td>\n",
    "    <td>Maximize</td> \n",
    "    <td>$I(X_i;Y)$</td>\n",
    "    <td>$I(X_j;Y |X_i)$</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><b>QUBO</b>: $\\sum_i^N q_ix_i + \\sum_{i<j}^N q_{i,j}x_i  x_j$</td>\n",
    "    <td>Minimize</td> \n",
    "    <td>$q_ix_i$</td>\n",
    "    <td>$q_{i,j}x_ix_j$</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Because the quantum computer seeks to minimize the programmed problem, these terms of the feature-selection formula should be set negative in the QUBO.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent a choice of $k$ features out of a set of $n$, it makes sense that the assignments of $x_1,...,x_N$, which represent solutions to the problem, set $x_i$ to $1$ if feature $X_i$ should be selected and to $0$ if not. \n",
    "\n",
    "With solutions encoded this way, you can represent the QUBO in matrix format, \n",
    "\n",
    "$\\mathbf{x}^T \\mathbf{Q x}$\n",
    "\n",
    "where $\\mathbf Q$ is an $n$ X $n$ matrix and $\\mathbf{x}$ is an $n$ x $1$ matrix (a vector) that should have $k$ ones representing the best features. To map the approximation above to a QUBO, the diagonal elements of $\\mathbf Q$, representing linear coefficients, should minimize MI between the output and each feature, $Q_{ii} \\leftarrow -I(X_i;Y)$, and non-diagonal elements, representing quadratic elements of $\\mathbf Q$, should minimize for pairs of features that complement information about the output variable rather than provide redundant information, $Q_{ij} \\leftarrow -I(X_j;Y |X_i)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A QUBO on the Toy Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a BQM and set the linear coefficients: the MI between `out` and each potential feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dimod\n",
    "bqm = dimod.BinaryQuadraticModel.empty(dimod.BINARY)\n",
    "\n",
    "for column in toy.columns:\n",
    "\n",
    "    if column == 'out':\n",
    "        continue\n",
    "\n",
    "    mi = mutual_information(prob(toy[['out', column]].values), 1)\n",
    "    bqm.add_variable(column, -mi)\n",
    "\n",
    "print(bqm.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the quadratic coefficients: the MI between `out` and each potential feature conditional on the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f0, f1 in itertools.combinations(['in1', 'in2', 'in3'], 2):\n",
    "    cmi[(f0, f1)] = conditional_mutual_information(prob(toy[['out', f0, f1]].values), 1, 2)\n",
    "    mi = conditional_mutual_information(prob(toy[['out', f0, f1]].values), 1, 2)\n",
    "    bqm.add_interaction(f0, f1, -mi)\n",
    "\n",
    "bqm.normalize()\n",
    "\n",
    "print(bqm.quadratic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dimod.reference.samplers.ExactSolver()\n",
    "\n",
    "result = sampler.sample(bqm)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best solution is when all three input signals are used. Next best, when only two are used, include `in3`.\n",
    "\n",
    "At this point, the mapping does not include any requirement on the number of selected features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalizing $len(s) != k$\n",
    "How do you represent the requirement that only $k$ features be selected? By penalizing solutions with more or less than $k$ ones:\n",
    "\n",
    "$P = \\alpha \\sum_{i=1}^n ( x_i - k)^2$ \n",
    "\n",
    "where $P$ is the penalty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "bqm.update(dimod.generators.combinations(bqm.variables, k, strength=4))\n",
    "\n",
    "result = sampler.sample(bqm)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Application: Predicting Survival of Titanic Passangers\n",
    "This example illustrates the MIQUBO method by finding an optimal feature set for predicting survival of Titanic passengers. It uses records provided in file formatted_titanic.csv, which is a feature-engineered version of a public database of passenger information recorded by the ship's crew (in addition to a column showing survival for each passenger, it contains information on gender, title, class, port of embarkation, etc). Its output is a ranking of subsets of features that have\n",
    "high MI with the variable of interest (survival) and low redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"data\\\\formatted_titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "features = list(set(titanic.columns).difference(('survived',)))\n",
    "for feature in features:\n",
    "    scores[feature] = mutual_information(prob(titanic[['survived', feature]].values), 0)\n",
    "\n",
    "labels, values = zip(*sorted(scores.items(), key=lambda pair: pair[1], reverse=True))\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.title(\"Mutual Information\")\n",
    "plt.ylabel('MI Between Survival and Feature')\n",
    "plt.xticks(np.arange(len(labels)), labels, rotation=90)\n",
    "plt.bar(np.arange(len(labels)), values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = 8\n",
    "\n",
    "titanic = titanic[[column[0] for column in sorted(scores.items(), key=lambda pair: pair[1], reverse=True)][0:keep] + [\"survived\"]]\n",
    "features = list(set(titanic.columns).difference(('survived',)))\n",
    "print(\"Submitting for {} features: {}\".format(keep, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bqm = dimod.BinaryQuadraticModel.empty(dimod.BINARY)\n",
    "\n",
    "# add the features\n",
    "for feature in features:\n",
    "    mi = mutual_information(prob(titanic[['survived', feature]].values), 0)\n",
    "    bqm.add_variable(feature, -mi)\n",
    "\n",
    "\n",
    "for f0, f1 in itertools.combinations(features, 2):\n",
    "    mi = conditional_mutual_information(prob(titanic[['survived', f0, f1]].values), 0, 2)\n",
    "    bqm.add_interaction(f0, f1, -mi)\n",
    "\n",
    "bqm.normalize()  # to -1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dwave.embedding.chimera import find_clique_embedding\n",
    "from dwave.system import DWaveSampler, FixedEmbeddingComposite\n",
    " \n",
    "qpu_sampler = DWaveSampler(solver={'qpu': True})\n",
    "\n",
    "embedding = find_clique_embedding(bqm.variables,\n",
    "                                  16, 16, 4,  # size of the chimera lattice\n",
    "                                  target_edges=qpu_sampler.edgelist)\n",
    "sampler = FixedEmbeddingComposite(qpu_sampler, embedding)\n",
    "\n",
    "print(\"Longest chain length is\", max(len(x) for x in embedding.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads = 10000\n",
    "selected_features = np.zeros((len(features), len(features)))\n",
    "for k in range(1, len(features) + 1):\n",
    "    print(\"Submitting for k = {} features\".format(k))\n",
    "    kbqm = bqm.copy()\n",
    "    kbqm.update(dimod.generators.combinations(features, k, strength=4))\n",
    "    \n",
    "    sample = sampler.sample(kbqm, num_reads=reads, chain_strength=6).first.sample\n",
    "   \n",
    "    for fi, f in enumerate(features):\n",
    "        selected_features[k-1, fi] = sample[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_axes([0.1, 0.3, .9, .7])\n",
    "ax.set_title(\"Best Feature Selection\")\n",
    "ax.set_ylabel('Number of Selected Features')\n",
    "ax.set_xticks(np.arange(len(features)))\n",
    "ax.set_xticklabels(features, rotation=90)\n",
    "ax.set_yticks(np.arange(len(features)))\n",
    "ax.set_yticklabels(np.arange(1, len(features)+1))\n",
    "# Set a grid on minor ticks\n",
    "ax.set_xticks(np.arange(-0.5, len(features)), minor=True)\n",
    "ax.set_yticks(np.arange(-0.5, len(features)), minor=True)\n",
    "ax.grid(which='minor', color='black')\n",
    "\n",
    "ax.imshow(selected_features, cmap=colors.ListedColormap(['white', 'red']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection and Mutual Information\n",
    "This section explains the math of mutual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described above, to select the :math:`k` most relevant features, you might maximize\n",
    ":math:`I({X_s}; Y)`, the MI between a set of :math:`k` features, :math:`X_s`, and the\n",
    "variable of interest, :math:`Y`. Given :math:`N` features out of which you select\n",
    ":math:`k`, maximize mutual information, I, as\n",
    "\n",
    "${X_1, X_2, ...X_k} = \\arg \\max I(X_k; Y)$\n",
    "\n",
    "by expanding,\n",
    "\n",
    "$I(X_k;Y) = N^{-1} \\sum_i \\left\\{ I(X_i;Y) + I(X_{k(i)};Y|X_i) \\right\\}$\n",
    "\n",
    "Approximate the second term by assuming conditional independence:\n",
    "\n",
    "$I(X_k;Y|X_i) \\approx \\sum_{j \\in X_k(i)} I(X_j;Y|X_i)$\n",
    "\n",
    "Using the following equations for Shannon entropy,\n",
    "\n",
    "$H(X) = -\\sum_x P(x)\\mathrm{log}P(x)\n",
    "    H(X|Y) = H(X,Y)-H(Y)$\n",
    "    \n",
    "You can then calculate all these terms as follows:\n",
    "\n",
    ".. math::\n",
    "     I(X;Y) = H(X)-H(X|Y)\n",
    "     I(X;Y|Z) = H(X|Z)-H(X|Y,Z)\n",
    "The approximated equation for MI can now be formed as a QUBO:\n",
    "\n",
    ".. math:\n",
    "    {X_1, X_2, ...X_k} = \\arg \\max \\left\\{MI - Penalty}\n",
    "where the penalty is some multiple of :math:`\\sum_{i} (x_i - k)^2` that enforces\n",
    "the constraint of :math:`k` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
